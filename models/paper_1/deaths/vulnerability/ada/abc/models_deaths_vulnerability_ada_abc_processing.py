from abc import ABC, abstractmethod
import os
from typing import NoReturn

import numpy as np
import pandas as pd
from sklearn.metrics import f1_score, accuracy_score, mean_squared_error

from src.models.models_visualization import (SkLearnConfusionMatrix)

from src.base.files.standard_columns_names import (Time_StandardColumnNames, Scale_StandardColumnNames)


class AbstractBaseModels_Deaths_Vulnerability_ADA_Processing(ABC):
    _default_test_years_list = [[2001, 2006, 2011, 2016],
                                [2002, 2007, 2012, 2017],
                                [2003, 2008, 2013, 2018],
                                [2004, 2009, 2014],
                                [2005, 2010, 2015]]
    _default_regions = ['below_96', '96_197', 'above_197']

    def __init__(self, regions: list[str] = None, test_years_list: list[list[int]] = None):
        self.test_years_list = test_years_list
        self.regions = regions
        self.used_columns = self.x_variables + [self.y_variable]

    @property
    def filename(self) -> str:
        return self.__class__.__name__

    @property
    @abstractmethod
    def _rename_variables_dict(self) -> dict[str, str]:
        pass

    @property
    def rename_variables_dict(self) -> dict[str, str]:
        return self._rename_variables_dict

    @property
    @abstractmethod
    def _model_algorithm(self) -> str:
        pass

    @property
    def model_algorithm(self) -> str:
        return self._model_algorithm

    @property
    def _model_risk_component(self) -> str:
        return "vulnerability"

    @property
    def model_risk_component(self) -> str:
        return self._model_risk_component

    @property
    @abstractmethod
    def _model_vulnerability(self) -> str:
        pass

    @property
    def model_vulnerability(self) -> str:
        return self._model_vulnerability

    @property
    @abstractmethod
    def _x_variables(self) -> list[str]:
        pass

    @property
    def x_variables(self) -> list[str]:
        return self._x_variables

    @property
    @abstractmethod
    def _y_variable(self) -> str:
        pass

    @property
    def y_variable(self) -> str:
        return self._y_variable

    @property
    @abstractmethod
    def _plot_title_suffix(self):
        pass

    @property
    def plot_title_suffix(self):
        return self._plot_title_suffix

    @property
    def test_years_list(self):
        return self._test_years_list

    @test_years_list.setter
    def test_years_list(self, value):
        if value is None:
            self._test_years_list = self._default_test_years_list
        else:
            self._test_years_list = value

    @property
    def regions(self):
        return self._regions

    @regions.setter
    def regions(self, value):
        if value is None:
            self._regions = self._default_regions
        else:
            self._regions = value

    def subset_test_years_filename(self, subset_region: str, subset_test_years: list[list], filename_suffix: str):
        return (f"{self.model_algorithm}_{subset_region}_model_{str(subset_test_years[0])[-1]}_"
                f"{self.model_risk_component}_{filename_suffix}")

    def subset_test_year_filename(self, subset_region: str, subset_year: int, filename_suffix: str):
        return f"{self.model_algorithm}_{subset_region}_{subset_year}_{self.model_risk_component}_{filename_suffix}"

    def subset_region_filename(self, subset_region: str, filename_suffix: str):
        return f"{self.model_algorithm}_{subset_region}_{self.model_risk_component}_{filename_suffix}"

    @staticmethod
    def split_train_test(df_complete_features, subset_region: str,
                         subset_test_years: list[int]) -> tuple[pd.DataFrame, pd.DataFrame]:

        df_raw = df_complete_features.copy()
        df_raw = df_raw.query(f"{Scale_StandardColumnNames().RCDD} == '{subset_region}'").astype(np.float64)
        df_copy = df_raw.copy()

        train_data = df_copy.query(f"{Time_StandardColumnNames().year} not in {subset_test_years}")
        test_data = df_copy.query(f"{Time_StandardColumnNames().year} in {subset_test_years}")

        return train_data, test_data

    def results_std_classification(self, path_in: str) -> pd.DataFrame:
        df_raw = pd.read_parquet(path_in).copy()

        df_raw['model'] = 0
        for test_years in self.test_years_list:
            df_raw.loc[df_raw.index.get_level_values(Time_StandardColumnNames().year).isin(test_years),
                       'model'] = test_years[0]

        df_raw['std_actual'] = df_raw.groupby([Scale_StandardColumnNames().RCDD, Time_StandardColumnNames().year]
                                              )[self.y_variable].transform('std')
        df_raw['std_prediction'] = df_raw.groupby([Scale_StandardColumnNames().RCDD, Time_StandardColumnNames().year]
                                                  )['prediction'].transform('std')

        choices = ["below_-1.5_std", "-1.5_-0.5_std", "-0.5_0.5_std", "0.5_1.5_std", "above_1.5_std"]

        for var, std in zip([self.y_variable, 'prediction'], ['std_actual', 'std_prediction']):
            classes = [
                df_raw[var] < -1.5 * df_raw[std],
                df_raw[var].between(-1.5 * df_raw[std], -0.5 * df_raw[std], inclusive='left'),
                df_raw[var].between(-0.5 * df_raw[std], 0.5 * df_raw[std], inclusive='left'),
                df_raw[var].between(0.5 * df_raw[std], 1.5 * df_raw[std], inclusive='left'),
                df_raw[var] >= 1.5 * df_raw[std]]

            df_raw.loc[:, f'{std}_classification'] = np.select(classes, choices)

        return df_raw

    def make_confusion_matrix(self, path_in: str, sub_path_out: str) -> NoReturn:

        df_raw = pd.read_parquet(path_in)
        df_copy = df_raw.copy()

        for region in self.regions:
            for test_years in self.test_years_list:
                df_query = df_copy.query(f"model == {test_years[0]} & {Scale_StandardColumnNames().RCDD} == '{region}'")

                matrix_f1_score = f1_score(y_true=df_query['std_actual_classification'],
                                           y_pred=df_query['std_prediction_classification'], average='weighted')
                matrix_accuracy = accuracy_score(y_true=df_query['std_actual_classification'],
                                                 y_pred=df_query['std_prediction_classification'])

                cm_fig = SkLearnConfusionMatrix(y_true_data=df_query['std_actual_classification'],
                                                         y_pred_data=df_query['std_prediction_classification'],
                                                         labels=["below_-1.5_std", "-1.5_-0.5_std", "-0.5_0.5_std",
                                                                 "0.5_1.5_std", "above_1.5_std"])

                cm_fig.add_title(title=f"Confusion matrix for {region} for the {test_years[0]} model \n"
                                       f"Accuracy: {round(matrix_accuracy, 3)} \n"
                                       f"Weighted F1 score: {round(matrix_f1_score, 3)}")

                tick_labels = ['Below\n-1.5 std', 'Between\n-1.5 and\n-0.5 std', 'Between\n-0.5 and\n0.5 std',
                               'Between\n0.5 and\n1.5 std', 'Above\n1.5 std']
                cm_fig.add_xaxis_tick_labels(x_axis_tick_labels=tick_labels)
                cm_fig.add_yaxis_tick_labels(y_axis_tick_labels=tick_labels)

                cm_fig.save_figure(path_out=os.path.join(sub_path_out, region),
                                   filename_out=f"{self.model_algorithm}_{region}_model_{str(test_years[0])[-1]}_"
                                                f"{self.model_risk_component}_confusion_matrix")

    def results_rmse(self, path_in: str) -> pd.DataFrame:
        def _rmse_stats(x):
            rmse_function = mean_squared_error(x[self.y_variable], x['prediction'], squared=False)
            min_truth_function = x[self.y_variable].min()
            max_truth_function = x[self.y_variable].max()
            min_pred_function = x['prediction'].min()
            max_pred_function = x['prediction'].max()
            std_truth = x[self.y_variable].std()
            std_pred = x['prediction'].std()
            matrix_f1_score = f1_score(y_true=x['std_actual_classification'],
                                       y_pred=x['std_prediction_classification'], average='weighted')
            matrix_accuracy = accuracy_score(y_true=x['std_actual_classification'],
                                             y_pred=x['std_prediction_classification'])

            return pd.Series({'rmse': rmse_function, 'min_actual': min_truth_function, 'max_actual': max_truth_function,
                              'min_pred': min_pred_function, 'max_pred': max_pred_function, 'std_actual': std_truth,
                              'std_pred': std_pred, 'matrix_accuracy': matrix_accuracy,
                              'matrix_f1_score': matrix_f1_score})

        df_raw = pd.read_parquet(path_in).copy()
        df_gb = df_raw.groupby(['model', Scale_StandardColumnNames().RCDD]
                               )[[self.y_variable, 'prediction', 'std_actual_classification',
                                  'std_prediction_classification']].apply(_rmse_stats)

        return df_gb.round(4)
