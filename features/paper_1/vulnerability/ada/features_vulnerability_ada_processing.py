from abc import ABC, abstractmethod
import pandas as pd

from src.base.files.standard_columns_names import (Time_StandardColumnNames, Scale_StandardColumnNames)
from src.features.paper_1.vulnerability.ada.features_vulnerability_ada_columns import (
    AbstractFeaturesVulnerabilityVariables, Features_Vulnerability_Deaths_Variables_F1)

from src.helpers.census_computation import compute_censuses_from_year_interval, compute_census_from_year
from src.helpers.pd_operation import (interpolate_df, add_aggregate_sum_column, add_rate_column)


class AbstractFeatures_Vulnerability_ADA_Processing(ABC):

    def __init__(self, year_start: int, year_end: int, month_start: int, month_end: int,
                 week_start: int, week_end: int):
        self.year_start = year_start
        self.year_end = year_end
        self.week_start = week_start
        self.week_end = week_end
        self.month_start = month_start
        self.month_end = month_end
        self.census_year_start = compute_census_from_year(year=year_start)
        self.census_year_end = compute_census_from_year(year=year_end)

    @property
    def filename(self) -> str:
        return f"{self.__class__.__name__}_{self.year_start}_{self.year_end}"

    @property
    @abstractmethod
    def _features_variables(self) -> AbstractFeaturesVulnerabilityVariables:
        pass

    @property
    def features_variables(self) -> AbstractFeaturesVulnerabilityVariables:
        return self._features_variables

    def select_daymet_variables(self, daymet_parquet_file: str) -> pd.DataFrame:
        df_raw = pd.read_parquet(daymet_parquet_file, columns=self.features_variables.daymet).copy()

        df_date_filtered = df_raw.query(f"{self.month_start}"
                                        f" <= {Time_StandardColumnNames().date}.dt.month <= "
                                        f"{self.month_end}")

        df_gb = df_date_filtered.groupby([Scale_StandardColumnNames().ADAUID, Scale_StandardColumnNames().RCDD,
                                          df_date_filtered.index.get_level_values(Time_StandardColumnNames().date).year,
                                          Time_StandardColumnNames().census]).mean().round(2)

        df_gb.index = df_gb.index.set_names({Time_StandardColumnNames().date: Time_StandardColumnNames().year})

        return df_gb

    def select_census_age_variables(self, census_parquet_file: str, census_year_start: int,
                                    census_year_end: int) -> pd.DataFrame:
        df_raw = pd.read_parquet(census_parquet_file, columns=self.features_variables.census_age).copy()

        # Add a year columns that will be extended in the interpolation operation
        df_raw[Time_StandardColumnNames().year] = df_raw.index.get_level_values(Time_StandardColumnNames().census)

        df_scale_gb = df_raw.groupby([Scale_StandardColumnNames().ADAUID, Scale_StandardColumnNames().RCDD,
                                      Time_StandardColumnNames().census, Time_StandardColumnNames().year]).sum()

        censuses_year = compute_censuses_from_year_interval(year_start=census_year_start,
                                                                               year_end=census_year_end)

        censuses_intervals = [(censuses_year[index], censuses_year[index + 1])
                              for index, census_year in enumerate(censuses_year) if census_year != censuses_year[-1]]

        df_list = []

        for census_interval in censuses_intervals:
            df_start = df_scale_gb.loc[df_scale_gb.index.get_level_values(Time_StandardColumnNames().census) ==
                                       census_interval[0]].reset_index(Time_StandardColumnNames().year)
            # Can't substract 2 df with multiindex, must drop the time index and keep only the scale RCDD
            df_end = df_scale_gb.loc[df_scale_gb.index.get_level_values(Time_StandardColumnNames().census) ==
                                     census_interval[-1]].reset_index(
                [Time_StandardColumnNames().census, Time_StandardColumnNames().year], drop=True)

            df_partial = interpolate_df(df_start=df_start, df_end=df_end, year_start=census_interval[0],
                                                     year_end=census_interval[-1])

            df_list.append(df_partial)

        df_out = pd.concat(df_list)

        return df_out.set_index(Time_StandardColumnNames().year, append=True)

    def select_census_socioeco_variables(self, census_parquet_file: str, census_year_start: int,
                                         census_year_end: int) -> pd.DataFrame:
        df_raw = pd.read_parquet(census_parquet_file, columns=self.features_variables.census_socioeco).copy()

        # Add a year columns that will be extended in the interpolation operation
        df_raw[Time_StandardColumnNames().year] = df_raw.index.get_level_values(Time_StandardColumnNames().census)

        df_scale_gb = df_raw.groupby([Scale_StandardColumnNames().ADAUID, Scale_StandardColumnNames().RCDD,
                                      Time_StandardColumnNames().census, Time_StandardColumnNames().year]).sum()

        censuses_year = compute_censuses_from_year_interval(year_start=census_year_start,
                                                                               year_end=census_year_end)
        # The 2011 census doesn't have any socioeconomic variables...
        valid_censuses_year = [census_year for census_year in censuses_year if census_year != 2011]

        censuses_intervals = [(valid_censuses_year[index], valid_censuses_year[index + 1])
                              for index, census_year in enumerate(valid_censuses_year)
                              if census_year != valid_censuses_year[-1]]

        df_list = []

        for census_interval in censuses_intervals:
            df_start = df_scale_gb.loc[df_scale_gb.index.get_level_values(Time_StandardColumnNames().census) ==
                                       census_interval[0]].reset_index(Time_StandardColumnNames().year)
            # Can't substract 2 df with multiindex, must drop the time index and keep only the scale RCDD
            df_end = df_scale_gb.loc[df_scale_gb.index.get_level_values(Time_StandardColumnNames().census) ==
                                     census_interval[-1]].reset_index(
                [Time_StandardColumnNames().census, Time_StandardColumnNames().year], drop=True)

            df_partial = interpolate_df(df_start=df_start, df_end=df_end, year_start=census_interval[0],
                                                     year_end=census_interval[-1])

            df_list.append(df_partial)

        df_out = pd.concat(df_list)
        df_out = df_out.reset_index()

        # Because the 2011 census was not used to interpolate, must add it back to the dataframe
        df_out.loc[(2011 <= df_out[Time_StandardColumnNames().year]) &
                   (df_out[Time_StandardColumnNames().year] <= 2015), Time_StandardColumnNames().census] = 2011

        return df_out.set_index([Scale_StandardColumnNames().ADAUID, Scale_StandardColumnNames().RCDD,
                                 Time_StandardColumnNames().census, Time_StandardColumnNames().year])

    def select_outcomes_variables(self, outcomes_parquet_file: str) -> pd.DataFrame:
        df_raw = pd.read_parquet(outcomes_parquet_file, columns=self.features_variables.outcomes).copy()

        df_date_filtered = df_raw.query(f"{self.month_start}"
                                        f" <= {Time_StandardColumnNames().date}.dt.month <= "
                                        f"{self.month_end}")

        df_gb = df_date_filtered.groupby([Scale_StandardColumnNames().ADAUID, Scale_StandardColumnNames().RCDD,
                                          df_date_filtered.index.get_level_values(Time_StandardColumnNames().date).year,
                                          Time_StandardColumnNames().census]).sum()

        df_gb.index = df_gb.index.set_names({Time_StandardColumnNames().date: Time_StandardColumnNames().year})

        return df_gb

    def select_canue_variables(self, canue_parquet_file: str) -> pd.DataFrame:
        df_raw = pd.read_parquet(canue_parquet_file, columns=self.features_variables.canue).copy()

        df_date_filtered = df_raw.query(f"{self.month_start} <= time_month <= {self.month_end}")

        return df_date_filtered.groupby([Scale_StandardColumnNames().ADAUID, Scale_StandardColumnNames().RCDD,
                                         Time_StandardColumnNames().year, Time_StandardColumnNames().census]
                                        ).mean().round(1).dropna()

    def select_ndvi_variables(self, ndvi_parquet_file: str) -> pd.DataFrame:
        df_raw = pd.read_parquet(ndvi_parquet_file, columns=self.features_variables.ndvi).copy()

        return df_raw.groupby([Scale_StandardColumnNames().ADAUID, Scale_StandardColumnNames().RCDD,
                               Time_StandardColumnNames().census]).sum().round(1).dropna()

    @staticmethod
    def concat_daymet_outcome(daymet_parquet_file: str, outcome_parquet_file: str) -> pd.DataFrame:
        df_daymet_raw = pd.read_parquet(daymet_parquet_file).copy()
        df_outcome_raw = pd.read_parquet(outcome_parquet_file).copy()

        # Very important to do a left merge on the daymet, because we are guaranteed to have a date for every day, but
        # it can be possible to have no entry for a date for the outcome
        df_merge = df_daymet_raw.merge(df_outcome_raw, how='left', left_index=True, right_index=True)

        return df_merge.fillna(0)

    def concat_census(self, census_age_parquet_file: str, census_socioeco_parquet_file: str) -> pd.DataFrame:
        df_census_age_raw = pd.read_parquet(census_age_parquet_file).copy()
        df_census_socioeco_raw = pd.read_parquet(census_socioeco_parquet_file).copy()

        df_merge = df_census_age_raw.merge(df_census_socioeco_raw, how='inner', left_index=True, right_index=True)

        return df_merge.query(f"{self.year_start} "
                              f"<= {Time_StandardColumnNames().year} <= "
                              f"{self.year_end}")

    @staticmethod
    def concat_variables(census_parquet_file: str, daymet_outcome_parquet_file: str, ndvi_parquet_file: str,
                         canue_parquet_file: str):
        df_census = pd.read_parquet(census_parquet_file).copy()
        df_daymet_outcome = pd.read_parquet(daymet_outcome_parquet_file).copy()
        df_ndvi = pd.read_parquet(ndvi_parquet_file).copy()
        df_canue = pd.read_parquet(canue_parquet_file).copy()

        df_ndvi_canue = df_ndvi.merge(df_canue, how='inner', left_index=True, right_index=True)
        df_census_ndvi_canue = df_ndvi_canue.merge(df_census, how='inner', left_index=True, right_index=True)

        return df_daymet_outcome.merge(df_census_ndvi_canue, how='inner', left_index=True, right_index=True)

    @abstractmethod
    def standardize_format(self, concat_variables_parquet_file: str) -> pd.DataFrame:
        pass

    @abstractmethod
    def features_census_stats(self, standardize_format_parquet_file: str):
        pass

    @abstractmethod
    def features_study_stats(self, standardize_format_parquet_file: str):
        pass


class Features_Deaths_Vulnerability_ADA_Processing_F1(AbstractFeatures_Vulnerability_ADA_Processing):

    @property
    def _features_variables(self) -> Features_Vulnerability_Deaths_Variables_F1:
        return Features_Vulnerability_Deaths_Variables_F1()

    def standardize_format(self, concat_variables_parquet_file: str) -> pd.DataFrame:
        df_raw = pd.read_parquet(concat_variables_parquet_file)

        df_agg = add_aggregate_sum_column(df=df_raw, agg_dict=self._features_variables.age_agg_dict)

        # Factor to have the daily deaths rate per 10,000 people
        scale_factor = 10000 / ((self.week_end - self.week_start + 1) * 7)

        df_processed = add_rate_column(df=df_agg, var_to_pct='dx', var_col_tot='census_Pop_Tot',
                                                    out_suffix='rate', rounding=4, scale_factor=scale_factor,
                                                    drop_in_col=False)

        pct_dict = {'census_Age_Tot_tot': 'census_Age', 'census_Pop_Tot': 'census_Pop',
                    'census_Household_Tot': 'census_Household', 'ndvi_superficie_tot': 'ndvi'}

        for var_tot, var_col in pct_dict.items():
            df_processed = add_rate_column(df=df_processed, var_to_pct=var_col, var_col_tot=var_tot,
                                                        out_suffix='pct', scale_factor=100)

        for variable in self.features_variables.outcomes:
            variable_rate = f"{variable}_rate"
            df_processed[f"region_mean_{variable_rate}"] = df_processed.groupby(
                [Time_StandardColumnNames().year, Scale_StandardColumnNames().RCDD])[variable_rate].transform('mean'
                                                                                                             ).round(4)

            df_processed[f"delta_mean_{variable_rate}"] = df_processed[variable_rate].sub(
                df_processed[f"region_mean_{variable_rate}"]).round(4)

        return df_processed.query('census_Age_Tot_tot >= 1000').dropna()

    def features_census_stats(self, standardize_format_parquet_file: str):
        df_raw = pd.read_parquet(standardize_format_parquet_file)
        df_copy = df_raw.copy()
        df_stats = df_copy.groupby([Scale_StandardColumnNames().RCDD, Time_StandardColumnNames().census]).mean()
        return df_stats.round(4)

    def features_study_stats(self, standardize_format_parquet_file: str):
        df_raw = pd.read_parquet(standardize_format_parquet_file)
        df_copy = df_raw.copy()
        df_stats = df_copy.groupby([Scale_StandardColumnNames().RCDD]).mean()
        return df_stats.round(4)
