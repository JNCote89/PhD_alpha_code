from abc import ABC, abstractmethod
from dataclasses import dataclass
import os

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from src.base.files.metadata_datacls import TimesMetadata
from src.base.files.metadata_mixins import TimesMetadataMixin
from src.base.files.files_abc import AbstractFeaturesFile

from src.features.paper_2.features_visualization import NDVI_Household_Density_Plots

from src.base.files_manager.files_path import QGISDataPaths
from src.base.files.standard_columns_names import Time_StandardColumnNames, Scale_StandardColumnNames
from src.features.paper_2.features_abc_processed_files import AbstractFeatures_ProcessedFile
from src.helpers.pd_operation import (interpolate_df, add_time_index, add_aggregate_sum_column, add_moving_avg_column,
                                      add_rate_column, recast_multiindex)

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 1000)
pd.set_option('display.width', 1600)


class Features_Vulnerability_ADA_ProcessedFile_F1(AbstractFeatures_ProcessedFile):

    @property
    def _times_metadata(self) -> TimesMetadata:
        return TimesMetadata(default_year_start=2001, default_year_end=2018, default_month_start=5, default_month_end=9,
                             default_week_start=20, default_week_end=38)

    @property
    def _filename(self) -> str:
        return f"Features_ADA_{self.year_start}_{self.year_end}_A"

    @property
    def _valid_ADA_list(self) -> [int]:
        return pd.read_csv(QGISDataPaths().load_file(sub_dir=os.path.join("Results", "Limits", "Mtl_4326"),
                                                     filename="valid_ADA_Mtl_16.csv"))['ADAUID'].tolist()

    @property
    def _groupby_scale(self) -> list[str]:
        return ['scale_ADAUID']

    @property
    def _column_names(self) -> [dataclass]:
        pass

    def add_features_variables_absolute(self, complete_parquet_file: str):
        # Important to detect heatwaves and other features
        gb = ['scenario_ssp', 'scenario_aging', 'time_year', 'time_date', 'time_census', 'scale_ADAUID']

        df = pd.read_parquet(complete_parquet_file).copy()

        df_aggregate_age = add_aggregate_sum_column(
            df=df,
            agg_dict={'census_Age_Tot_65_over': ['census_Age_Tot_65_69', 'census_Age_Tot_70_74',
                                                 'census_Age_Tot_75_79', 'census_Age_Tot_80_84',
                                                 'census_Age_Tot_85_over'],
                      'census_Age_Tot_75_over': ['census_Age_Tot_75_79', 'census_Age_Tot_80_84',
                                                 'census_Age_Tot_85_over'],
                      'census_Age_Tot_65_74': ['census_Age_Tot_65_69', 'census_Age_Tot_70_74'],
                      'census_Age_Tot_75_84': ['census_Age_Tot_75_79', 'census_Age_Tot_80_84']},
            drop_agg_col=False)

        df_daymet_moving_avg = add_moving_avg_column(
            df=df_aggregate_age, variables_to_avg=['daymet_tmax'],
            window_length=[2, 3, 4])

        # Do not filter before, because the moving average needs the data prior the start of the study period.
        df_processed = df_daymet_moving_avg.query(f"{self.week_start} <= "
                                                  f"time_date.dt.isocalendar().week <= {self.week_end}")

        return df_processed

    def add_features_variables_percentage(self, complete_parquet_file: str):
        groupby_keys = ['time_year', 'time_census', 'scale_ADAUID']
        stats_col = ['population_above_30_pct_ndvi_300m', 'population_above_50_pct_ndvi_300m',
                     'census_Age_Tot_65_69', 'census_Age_Tot_70_74', 'census_Age_Tot_75_79', 'census_Age_Tot_80_84',
                     'census_Age_Tot_85_over', 'census_Age_Tot_65_over', 'census_Age_Tot_75_over',
                     'census_Age_Tot_65_74', 'census_Age_Tot_75_84', 'census_Pop_Tot', 'census_Pop_Lico_at',
                     'census_Pop_No_degree', 'census_Household_One_person', 'census_Pop_Renter',
                     'census_Household_Tot',
                     'census_Pop_Tot_region']

        df = pd.read_parquet(complete_parquet_file).query("census_Age_Tot_tot >= 1000 & "
                                                          "scenario_ssp == 'historical'").reset_index(
            ['scenario_ssp', 'scenario_aging'], drop=True).copy()

        df_base = df[['dx_tot_deaths', 'daymet_tmax_moving_avg_3',
                      'census_Pop_Tot']].query('daymet_tmax_moving_avg_3 < 28')
        df_hw = df[['dx_tot_deaths', 'daymet_tmax_moving_avg_3',
                    'census_Pop_Tot']].query('daymet_tmax_moving_avg_3 > 20')

        df_base_rate = add_rate_column(df=df_base, var_to_pct='dx',
                                       var_col_tot='census_Pop_Tot', out_suffix='base_rate',
                                       rounding=4, scale_factor=10000, drop_in_col=False).groupby(
            groupby_keys).mean().round(4)

        df_hw_rate = add_rate_column(df=df_hw, var_to_pct='dx',
                                     var_col_tot='census_Pop_Tot', out_suffix='hw_rate',
                                     rounding=4, scale_factor=10000, drop_in_col=False).groupby(
            groupby_keys).mean().round(4)

        df_merge_rate = pd.concat([df_base_rate['dx_tot_deaths_base_rate'],
                                   df_hw_rate['dx_tot_deaths_hw_rate']], axis=1)

        df_merge_rate['dx_tot_deaths_regional_base_rate'] = (
            df_merge_rate['dx_tot_deaths_base_rate'].groupby('time_year').transform('mean').round(4))

        df_merge_rate['delta_mean_dx_tot_deaths_rate'] = (
                df_merge_rate['dx_tot_deaths_hw_rate'] - df_merge_rate['dx_tot_deaths_regional_base_rate'])

        df_gb_mode = df[stats_col].groupby(groupby_keys, observed=True).agg(
            lambda x: pd.Series.mode(x, dropna=False)[0])

        df_concat = pd.concat([df_merge_rate, df_gb_mode], axis=1)

        pct_dict = {'census_Pop_Tot_region': ['census_Age', 'census_Pop', 'population_above',
                                              'census_Household_One_person']}

        for var_tot, var_cols in pct_dict.items():
            for var_col in var_cols:
                df_concat = add_rate_column(df=df_concat, var_to_pct=var_col, var_col_tot=var_tot, out_suffix='pct',
                                            scale_factor=100, drop_in_col=False, rounding=5)

        for col in stats_col:
            if col not in ['census_Pop_Tot_region', 'census_Household_Tot']:
                df_concat[f"delta_{col}_pct"] = (
                        df_concat[f"{col}_pct"] - (df_concat[f"{col}_pct"].groupby('time_year').transform('mean')
                                                   )).round(4)

        return df_concat

    def fill_missing_projections_values(self, complete_parquet_file: str):
        # If alternative scenario for ndvi
        return pd.read_parquet(complete_parquet_file)

    def standardize_format(self, complete_parquet_file: str):
        groupby_keys = ['time_year', 'time_census'] + self.groupby_scale
        df = pd.read_parquet(complete_parquet_file).copy()
        corr = df[['delta_mean_dx_tot_deaths_rate',
                   'delta_census_Age_Tot_65_74_pct',
                   'delta_census_Age_Tot_75_84_pct', 'delta_census_Age_Tot_85_over_pct',
                   'delta_census_Pop_No_degree_pct',
                   'delta_census_Pop_Lico_at_pct', 'delta_census_Household_One_person_pct',
                   'delta_census_Pop_Renter_pct',
                   'delta_population_above_30_pct_ndvi_300m_pct',
                   'delta_population_above_50_pct_ndvi_300m_pct']].corr()
        corr.to_csv('var_corr.csv')

        return df


class Features_Vulnerability_ADA_StatsFile(TimesMetadataMixin):

    @property
    def _times_metadata(self) -> TimesMetadata:
        return TimesMetadata(default_year_start=2001, default_year_end=2018, default_month_start=5, default_month_end=9,
                             default_week_start=20, default_week_end=38)

    def make_ndvi_ada_stats(self, ndvi_parquet_file: str):
        df = pd.read_parquet(ndvi_parquet_file).reset_index('time_census',
                                                            drop=True).query(f"{self.year_start} <= time_year "
                                                                             f"<= {self.year_end} &"
                                                                             f"census_Pop_Tot > 1000"
                                                                             ).copy()
        df_stats = df.groupby(['time_year'])[[col for col in df.columns if col.startswith('population_above')
                                              ]].describe().round(2)

        return df_stats


    def make_ndvi_household_density_plots(self, ndvi_parquet_file: str, path_out: str):
        df = pd.read_parquet(ndvi_parquet_file).query('time_year == 2018 & nb_units_ada > 500').copy()
        density = df['census_household_vegetation_density']
        percentage = (df['household_above_30_pct_ndvi_300m'] / df['nb_units_ada']) * 100

        ndvi_plots = NDVI_Household_Density_Plots(title='Percentage of households with minimum 30% \n'
                                                        'of vegetation within a 300m radius for 2018')
        ndvi_plots.make_plot(x_density=density, y_percentage=percentage)
        ndvi_plots.add_legend()

        ndvi_plots.save_figure(path_out=path_out, filename_out='ADA_NDVI_household_size_2018')


    def make_temperature_ada_stats(self, daymet_parquet_file: str):
        df = pd.read_parquet(daymet_parquet_file)

        df = df.query(f"{self.year_start} <= time_date.dt.year <= {self.year_end}").copy()

        df_stats = df.groupby(df.index.get_level_values('time_date').year)[['daymet_tmax']].describe()

        return df_stats

    def make_census_ada_stats(self, census_parquet_file: str):
        df = pd.read_parquet(census_parquet_file)
        df = df.query(f"{self.year_start} <= time_year <= {self.year_end}").copy()
        df_stats = df.groupby(['time_year'])[['census_Age_Tot_65_74_pct', 'census_Age_Tot_75_over_pct',
                                              'census_Pop_No_degree_pct', 'census_Pop_Lico_at_pct']].describe().round(2)

        return df_stats

    def make_death_ada_stats(self, death_parquet_file: str):
        df = pd.read_parquet(death_parquet_file)

        df = df.query(f"{self.year_start} <= time_year <= {self.year_end}").copy()
        df_stats = df.groupby(['time_year'])[['dx_tot_deaths_hw_rate']].describe().round(2)

        return df_stats

    def make_death_temperature_ada_stats(self, death_parquet_file: str):
        """To be refactored, final standardize file do no have temperature in this version"""
        df = pd.read_parquet(death_parquet_file)
        df = df.query(f"{self.year_start} <= time_year <= {self.year_end}").copy()

        df_mean_deaths_below_28 = df[df['daymet_tmax_moving_avg_3'] <= 28].groupby(['time_year', 'scale_ADAUID'])[
            'dx_tot_deaths_rate'].mean().round(2).rename('mean_deaths_below_28').to_frame()
        df_mean_deaths_below_28_description = df_mean_deaths_below_28.groupby('time_year').describe().round(2)

        df_mean_deaths_above_28 = df[df['daymet_tmax_moving_avg_3'] > 28].groupby(['time_year', 'scale_ADAUID'])[
            'dx_tot_deaths_rate'].mean().round(2).rename('mean_deaths_above_28').to_frame()
        df_mean_deaths_above_28_description = df_mean_deaths_above_28.groupby('time_year').describe().round(2)

        df_mean_deaths_above_30 = df[df['daymet_tmax_moving_avg_3'] > 30].groupby(['time_year', 'scale_ADAUID'])[
            'dx_tot_deaths_rate'].mean().round(2).rename('mean_deaths_above_30').to_frame()
        df_mean_deaths_above_30_description = df_mean_deaths_above_30.groupby('time_year').describe().round(2)

        df_mean_deaths_above_32 = df[df['daymet_tmax_moving_avg_3'] > 32].groupby(['time_year', 'scale_ADAUID'])[
            'dx_tot_deaths_rate'].mean().round(2).rename('mean_deaths_above_32').to_frame()
        df_mean_deaths_above_32_description = df_mean_deaths_above_32.groupby('time_year').describe().round(2)

        df_stats = pd.concat([df_mean_deaths_below_28_description, df_mean_deaths_above_28_description,
                              df_mean_deaths_above_30_description, df_mean_deaths_above_32_description], axis=1)
        return df_stats
